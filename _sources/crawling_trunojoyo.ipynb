{"cells":[{"cell_type":"markdown","source":["# Crawling PTA Trunojoyo"],"metadata":{"id":"Z7Lb2uITYmhg"}},{"cell_type":"markdown","source":["## Install Library BeautifulSoup"],"metadata":{"id":"ECaKTxrisC80"}},{"cell_type":"markdown","source":["Beautiful Soup adalah sebuah perangkat yang umumnya digunakan untuk melakukan crawling dan ekstraksi informasi dari dokumen HTML dan XML. Alat ini menawarkan metode yang sederhana untuk menjelajahi dan mengolah struktur dokumen HTML. Dengan Beautiful Soup, Anda dapat dengan mudah mencari elemen berdasarkan tag, atribut, dan teks yang terdapat dalam elemen tersebut."],"metadata":{"id":"SxcQXMO9sNn1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryWA3AV0Q5Oj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702463266032,"user_tz":-420,"elapsed":10449,"user":{"displayName":"Naufal Abdullah R.Z","userId":"05006711856112182730"}},"outputId":"1f8188c4-94df-4b33-bba6-94fb768b0c95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"]}],"source":["!pip install beautifulsoup4"]},{"cell_type":"markdown","source":["## Import Library"],"metadata":{"id":"6KP9-9Nys5JK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zon-pW4vRGHz"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import requests\n","import csv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24097,"status":"ok","timestamp":1702463290124,"user":{"displayName":"Naufal Abdullah R.Z","userId":"05006711856112182730"},"user_tz":-420},"id":"fB95fg4wU48B","outputId":"aa60a1a6-8764-4b7b-8296-e904d7dc49ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"RTGID3yqRuMs"},"source":["## Crawling Function"]},{"cell_type":"markdown","source":["Fungsi ini mengambil informasi dari halaman tugas akhir berdasarkan URL yang diberikan. Fungsi tersebut mengekstrak judul tugas akhir, nama penulis, nama dosen pembimbing, dan abstrak dari halaman tersebut."],"metadata":{"id":"-0bryjxxtHm0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHD47mEbR8Td"},"outputs":[],"source":["def get_data(url):\n","  data_page = []\n","  response = requests.get(url)\n","  if response.status_code == 200:\n","    page = BeautifulSoup(response.content)\n","    container_content = page.find('li', {\"data-id\" : \"id-1\"})\n","    header_page = container_content.find('div', {\"style\" : \"float:left; width:540px;\"})\n","\n","    #  Mengambil judul TA\n","    get_title = header_page.find('a', class_ = 'title').text.strip().split('\\r\\n')\n","    title = \" \".join(get_title)\n","    data_page.append(title)\n","\n","    # Penulis dan dosen pembimbing\n","    creator = header_page.find_all('span')\n","    for i in creator:\n","      splitting_text = i.text.strip().split(':')\n","      if splitting_text[0].strip() == 'Penulis':\n","        data_page.append(splitting_text[1].strip())\n","      elif splitting_text[0].strip() == 'Dosen Pembimbing I':\n","        data_page.append(splitting_text[1].strip())\n","      else:\n","        data_page.append(splitting_text[1].strip())\n","\n","    # Mengambil Abstrak\n","    get_abstrak = container_content.find('p', {\"align\" : \"justify\"}).text.strip().split('\\r\\n')\n","    abstrak = \" \".join(get_abstrak)\n","    data_page.append(abstrak)\n","\n","  return data_page\n"]},{"cell_type":"markdown","source":["Fungsi ini melakukan crawling atau pengambilan data dari halaman yang berisi daftar tugas akhir. Proses ini melibatkan pengambilan URL dari halaman tersebut, pencarian elemen-elemen dalam daftar tugas akhir, ekstraksi URL untuk setiap halaman detail tugas akhir, dan penerapan fungsi get_data() untuk mendapatkan informasi lebih lanjut."],"metadata":{"id":"5xQSFFVctjkn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPG6w2K4SDn8"},"outputs":[],"source":["def crawling_pta(url):\n","  hasil_crawling = []\n","  response = requests.get(url)\n","  if response.status_code == 200:\n","    page = BeautifulSoup(response.content)\n","    container_content = page.find('ul', class_= 'items')\n","    list_content = container_content.find_all('li')\n","\n","    for content in list_content:\n","      view_button = content.find('a', class_ = 'gray button').get('href')\n","      hasil_crawling.append(get_data(view_button))\n","\n","    next_page = page.find_all('a', class_='pag_button')\n","    for i in next_page:\n","      if i.text.strip() == '>':\n","        return (hasil_crawling, i.get('href'))"]},{"cell_type":"markdown","source":["Bagian ini mengeksekusi fungsi main dengan menyediakan URL dari halaman awal dan menentukan jumlah halaman yang ingin diambil."],"metadata":{"id":"GeTmgehottoy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zduxhwXQS2Q9"},"outputs":[],"source":["def main(url, num_page):\n","    try:\n","        content, next_page = crawling_pta(url)\n","\n","        for i in range(num_page - 1):\n","            more_content, next_page = crawling_pta(next_page)\n","            content += more_content\n","\n","        with open('/content/drive/MyDrive/PPW /hasil_crawling/crawling.csv', mode='w', newline='', encoding='utf-8') as file:\n","            writer = csv.writer(file)\n","            writer.writerow(['Judul', 'Penulis', 'Dosen Pembimbing I', 'Dosen Pembimbing II', 'Abstrak'])\n","            writer.writerows(content)\n","\n","        print(f'Data Di Simpan Ke crawling_pta.csv Dengan Jumlah : {len(content)} Data')\n","\n","    except Exception as e:\n","        print(f\"Terjadi kesalahan: {e}\")"]},{"cell_type":"markdown","source":["## Data disimpan ke csv"],"metadata":{"id":"zCLF-6asRNCV"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81818,"status":"ok","timestamp":1702463371937,"user":{"displayName":"Naufal Abdullah R.Z","userId":"05006711856112182730"},"user_tz":-420},"id":"kd1jFw5ZSqDu","outputId":"85ae43f4-1d54-452a-e669-e90b7604348c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data Di Simpan Ke crawling_pta.csv Dengan Jumlah : 50 Data\n"]}],"source":["main('https://pta.trunojoyo.ac.id/c_search/byprod/10', 10)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1JnTMfNPqMxFQOV4Kw7NVLb9E2AgA9cln","timestamp":1699618811620},{"file_id":"1FH4eOhr9VquNU9z5lITTIJYQyRcgJoB1","timestamp":1697675080419}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}